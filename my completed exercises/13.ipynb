{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The `tf.data` API is useful for reading in data gradually from your hard drive and preprocessing data, it revolves around `tf.data.Dataset` which represents a sequence of data items.\n",
    "\n",
    "2. Splitting the data across multiple files can help with shuffling the data, `tf.data.Dataset.list_files()` automatically shuffles the file paths and the `.interleave()` method with argument `cycle_length > 1` will read from multiple files simultaneously. It can also improve performance, setting `num_parallel_calls=tf.data.AUTOTUNE` TensorFlow will choose the right number of threads dynamically based on the available CPU.\n",
    "\n",
    "3. During training if your GPU utilization is very low, the input pipeline might be the bottleneck. You should call `.prefetch()` so that the dataset has a set number of batches ready to go in memory, or if the whole dataset can fit in memory, call `.cache()` after loading and preprocessing the data, but before shuffling, repeating, batching, and prefetching. Other approaches:\n",
    "    * read and preprocess the data with multiple threads in parallel,\n",
    "    * make sure your preprocessing code is optimized,\n",
    "    * save the dataset into multiple TFRecord files,\n",
    "    * if necessary perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training (TF Transform can help with this), &\n",
    "    * if necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough.\n",
    "\n",
    "4. You can save any binary data to a TFRecord file, however in practice *protobufs* are used.\n",
    "\n",
    "5. The `Example` protobuf format has the advantage that TensorFlow provides some operations to parse it (the `tf.io.parse`*`example()` functions) without you having to define your own format. It is sufficiently flexible to represent instances in most datasets.\n",
    "\n",
    "6. Compressing TFRecord files is useful if they need to be loaded via a network connection, e.g. from AWS S3. However, you shouldn't do this if you don't need to as decompressing the files could slow down training.\n",
    "\n",
    "7. Data preprocessing can take place in three ways:\n",
    "    1. when writing the data files\n",
    "        * **pros:** this will speed up training, the training data may also take up less space e.g. you apply dimensionality reduction\n",
    "        * **cons:** however you must make sure you apply the same preprocessing steps in production, its also not easy to try out different preprocessing steps, also not good for data augmentation\n",
    "    2. within the `tf.data` pipeline\n",
    "        * **pros:** its much easier to experiment with preprocessing steps & data augmentation, multithreading and prefetching can make it very efficient, you can use preprocessing layers in your `tf.data` pipeline and then reuse these layers when deploying your model to production\n",
    "        * **cons:** it will still slow down training, each instance will be preprocessed once per epoch (unless you can use `.cache`), must remember to apply the same preprocessing steps in production\n",
    "    3. preprocessing layers of your model\n",
    "        * **pros:** this is good for inference, as your model will be able to handle raw data, you will not run the risk of mismatch between your training preprocessing & inference preprocessing\n",
    "        * **cons:** it will slow down training, with each instance being processed multiple times\n",
    "\n",
    "8. Categorical features can be encoded using integers if there is a natural ordering or one-hot encoding. For text, where each token is a category, you have far too many categories to use one-hot so embeddings make much more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fashion MNIST data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.a. Writing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping train_set as directory is not empty.\n",
      "Skipping valid_set as directory is not empty.\n",
      "Skipping test_set as directory is not empty.\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "class_labels = (\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    ")\n",
    "\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(\n",
    "    buffer_size=60_000\n",
    ")\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "valid_size = 2048\n",
    "\n",
    "valid_set = train_set.take(valid_size)\n",
    "train_set = train_set.skip(valid_size)\n",
    "\n",
    "\n",
    "def serialize(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    protobuf_example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                \"image\": tf.train.Feature(\n",
    "                    bytes_list=tf.train.BytesList(value=[image_data.numpy()])\n",
    "                ),\n",
    "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return protobuf_example.SerializeToString()\n",
    "\n",
    "\n",
    "data_dir = Path(\"datasets/13/fashion_mnist\")\n",
    "num_shards = 5\n",
    "\n",
    "datasets = {\n",
    "    \"train_set\": train_set,\n",
    "    \"valid_set\": valid_set,\n",
    "    \"test_set\": test_set,\n",
    "}\n",
    "\n",
    "file_paths = dict()\n",
    "for dataset_name, dataset in datasets.items():\n",
    "\n",
    "    dataset_dir = data_dir / dataset_name\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_paths[dataset_name] = [\n",
    "        str(dataset_dir / f\"{dataset_name}-{i}-of-{num_shards}.tfrecord\")\n",
    "        for i in range(1, num_shards + 1)\n",
    "    ]\n",
    "\n",
    "    if not any(dataset_dir.iterdir()):\n",
    "        writers = [\n",
    "            tf.io.TFRecordWriter(file_path) for file_path in file_paths[dataset_name]\n",
    "        ]\n",
    "\n",
    "        for i, (image, label) in dataset.enumerate():\n",
    "            writers[i % num_shards].write(serialize(image, label))\n",
    "\n",
    "        for writer in writers:\n",
    "            writer.close()\n",
    "    else:\n",
    "        print(f\"Skipping {dataset_name} as directory is not empty.\")\n",
    "\n",
    "del (\n",
    "    X_test,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    test_set,\n",
    "    train_set,\n",
    "    valid_set,\n",
    "    data_dir,\n",
    "    dataset,\n",
    "    dataset_dir,\n",
    "    dataset_name,\n",
    "    datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized_example):\n",
    "\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    protobuf_example = tf.io.parse_single_example(\n",
    "        serialized_example, feature_description\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.io.parse_tensor(protobuf_example[\"image\"], out_type=tf.uint8),\n",
    "        tf.cast(protobuf_example[\"label\"], tf.uint8),\n",
    "    )\n",
    "\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "\n",
    "def preprocess(X, y):\n",
    "    return tf.reshape(norm_layer(X), (28, 28, 1)), y\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name, shuffle=False, sample=None):\n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(file_paths[dataset_name], num_parallel_reads=num_shards)\n",
    "        .map(parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(60_000 - valid_size)\n",
    "\n",
    "    if sample:\n",
    "        dataset = dataset.take(sample)\n",
    "\n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_dataset = get_dataset(\"train_set\", shuffle=True, sample=4096)\n",
    "valid_dataset = get_dataset(\"valid_set\")\n",
    "test_dataset = get_dataset(\"test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:34:45.500727: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2025-02-24 18:34:45.500740: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n",
      "2025-02-24 18:34:45.501090: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20/Unknown \u001b[1m7s\u001b[0m 182ms/step - accuracy: 0.1581 - loss: 4.8474"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:34:52.692526: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2025-02-24 18:34:52.692547: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    119/Unknown \u001b[1m24s\u001b[0m 171ms/step - accuracy: 0.2561 - loss: 2.7227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:35:09.777548: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:68] Profiler session collecting data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    120/Unknown \u001b[1m25s\u001b[0m 175ms/step - accuracy: 0.2568 - loss: 2.7162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:35:10.138459: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n",
      "2025-02-24 18:35:10.143331: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:147] Collecting XSpace to repository: models/TensorBoard/13/run_2025_02_24_18_34_45/train/plugins/profile/2025_02_24_18_35_10/Edwards-MacBook-Air.local.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    128/Unknown \u001b[1m26s\u001b[0m 176ms/step - accuracy: 0.2628 - loss: 2.6664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:35:11.717264: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/opt/miniconda3/envs/homl3/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 314ms/step - accuracy: 0.2635 - loss: 2.6605 - val_accuracy: 0.7003 - val_loss: 0.9595\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:35:29.209999: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 306ms/step - accuracy: 0.5553 - loss: 1.2611 - val_accuracy: 0.7576 - val_loss: 0.7290\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:36:08.305334: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 332ms/step - accuracy: 0.6559 - loss: 0.9230 - val_accuracy: 0.7866 - val_loss: 0.5665\n",
      "Epoch 4/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 344ms/step - accuracy: 0.7127 - loss: 0.7981 - val_accuracy: 0.7791 - val_loss: 0.5899\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:37:34.622469: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 350ms/step - accuracy: 0.7256 - loss: 0.7748 - val_accuracy: 0.8181 - val_loss: 0.5102\n",
      "Epoch 6/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 344ms/step - accuracy: 0.7660 - loss: 0.6529 - val_accuracy: 0.8471 - val_loss: 0.4422\n",
      "Epoch 7/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 328ms/step - accuracy: 0.7878 - loss: 0.6293 - val_accuracy: 0.8368 - val_loss: 0.4476\n",
      "Epoch 8/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 326ms/step - accuracy: 0.7910 - loss: 0.6121 - val_accuracy: 0.8312 - val_loss: 0.4618\n",
      "Epoch 9/100\n",
      "\u001b[1m  1/128\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 179ms/step - accuracy: 0.8125 - loss: 0.8640"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:40:26.835493: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 327ms/step - accuracy: 0.8131 - loss: 0.5832 - val_accuracy: 0.8526 - val_loss: 0.4036\n",
      "Epoch 10/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 323ms/step - accuracy: 0.8237 - loss: 0.5551 - val_accuracy: 0.8549 - val_loss: 0.4032\n",
      "Epoch 11/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 322ms/step - accuracy: 0.8324 - loss: 0.5357 - val_accuracy: 0.8563 - val_loss: 0.3944\n",
      "Epoch 12/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 318ms/step - accuracy: 0.8068 - loss: 0.5796 - val_accuracy: 0.8442 - val_loss: 0.4238\n",
      "Epoch 13/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 306ms/step - accuracy: 0.8236 - loss: 0.5069 - val_accuracy: 0.8576 - val_loss: 0.3753\n",
      "Epoch 14/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 319ms/step - accuracy: 0.8335 - loss: 0.5293 - val_accuracy: 0.8566 - val_loss: 0.3789\n",
      "Epoch 15/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 338ms/step - accuracy: 0.8341 - loss: 0.4939 - val_accuracy: 0.8619 - val_loss: 0.3923\n",
      "Epoch 16/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 335ms/step - accuracy: 0.8355 - loss: 0.4678 - val_accuracy: 0.8690 - val_loss: 0.3772\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:45:57.267546: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 336ms/step - accuracy: 0.8374 - loss: 0.5018 - val_accuracy: 0.8731 - val_loss: 0.3648\n",
      "Epoch 18/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 335ms/step - accuracy: 0.8475 - loss: 0.4638 - val_accuracy: 0.8804 - val_loss: 0.3370\n",
      "Epoch 19/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 316ms/step - accuracy: 0.8503 - loss: 0.4078 - val_accuracy: 0.8714 - val_loss: 0.3550\n",
      "Epoch 20/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 323ms/step - accuracy: 0.8581 - loss: 0.4388 - val_accuracy: 0.8780 - val_loss: 0.3294\n",
      "Epoch 21/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 324ms/step - accuracy: 0.8480 - loss: 0.4492 - val_accuracy: 0.8634 - val_loss: 0.3583\n",
      "Epoch 22/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 325ms/step - accuracy: 0.8612 - loss: 0.4242 - val_accuracy: 0.8821 - val_loss: 0.3306\n",
      "Epoch 23/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 326ms/step - accuracy: 0.8626 - loss: 0.4278 - val_accuracy: 0.8736 - val_loss: 0.3371\n",
      "Epoch 24/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 316ms/step - accuracy: 0.8481 - loss: 0.4340 - val_accuracy: 0.8822 - val_loss: 0.3252\n",
      "Epoch 25/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 326ms/step - accuracy: 0.8629 - loss: 0.3995 - val_accuracy: 0.8887 - val_loss: 0.3160\n",
      "Epoch 26/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 326ms/step - accuracy: 0.8553 - loss: 0.4231 - val_accuracy: 0.8856 - val_loss: 0.3188\n",
      "Epoch 27/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 306ms/step - accuracy: 0.8524 - loss: 0.4354 - val_accuracy: 0.8882 - val_loss: 0.2985\n",
      "Epoch 28/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 316ms/step - accuracy: 0.8674 - loss: 0.4024 - val_accuracy: 0.8749 - val_loss: 0.3377\n",
      "Epoch 29/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 315ms/step - accuracy: 0.8642 - loss: 0.4355 - val_accuracy: 0.8858 - val_loss: 0.3125\n",
      "Epoch 30/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 317ms/step - accuracy: 0.8723 - loss: 0.4140 - val_accuracy: 0.8842 - val_loss: 0.3101\n",
      "Epoch 31/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 312ms/step - accuracy: 0.8534 - loss: 0.4123 - val_accuracy: 0.8800 - val_loss: 0.3160\n",
      "Epoch 32/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 315ms/step - accuracy: 0.8689 - loss: 0.4267 - val_accuracy: 0.9013 - val_loss: 0.2826\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:56:53.180637: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 314ms/step - accuracy: 0.8724 - loss: 0.3664 - val_accuracy: 0.8915 - val_loss: 0.2996\n",
      "Epoch 34/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 317ms/step - accuracy: 0.8796 - loss: 0.3683 - val_accuracy: 0.8860 - val_loss: 0.3273\n",
      "Epoch 35/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 315ms/step - accuracy: 0.8834 - loss: 0.3531 - val_accuracy: 0.9004 - val_loss: 0.2783\n",
      "Epoch 36/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 311ms/step - accuracy: 0.8786 - loss: 0.3635 - val_accuracy: 0.8730 - val_loss: 0.3442\n",
      "Epoch 37/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 242ms/step - accuracy: 0.8771 - loss: 0.3965 - val_accuracy: 0.8980 - val_loss: 0.2794\n",
      "Epoch 38/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 157ms/step - accuracy: 0.8800 - loss: 0.3743 - val_accuracy: 0.8906 - val_loss: 0.3029\n",
      "Epoch 39/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 157ms/step - accuracy: 0.8683 - loss: 0.3665 - val_accuracy: 0.8912 - val_loss: 0.2908\n",
      "Epoch 40/100\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 167ms/step - accuracy: 0.8856 - loss: 0.3622 - val_accuracy: 0.8950 - val_loss: 0.2892\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 35.\n"
     ]
    }
   ],
   "source": [
    "DefaultConv2D = partial(\n",
    "    tf.keras.layers.Conv2D,\n",
    "    kernel_size=3,\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "        DefaultConv2D(filters=64, kernel_size=7),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        DefaultConv2D(filters=128),\n",
    "        DefaultConv2D(filters=128),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        DefaultConv2D(filters=256),\n",
    "        DefaultConv2D(filters=256),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=128, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=64, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            f\"models/TensorBoard/13/{strftime('run_%Y_%m_%d_%H_%M_%S')}\",\n",
    "            profile_batch=\"20,120\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Binary classification of the [Large Movie Review Dataset](https://homl.info/imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that parses a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"datasets/13/aclImdb\")\n",
    "\n",
    "\n",
    "def parse(file_path):\n",
    "    content = tf.io.read_file(file_path)\n",
    "\n",
    "    label = tf.strings.regex_replace(file_path, \".*/(pos|neg)/.*\", \"\\\\1\")\n",
    "    label = tf.cast(tf.equal(label, \"pos\"), tf.int32)\n",
    "\n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some basic analysis of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (\n",
    "    tf.data.Dataset.list_files(\n",
    "        [str(data_dir / f\"train/{class_}/*\") for class_ in [\"pos\", \"neg\"]]\n",
    "    )\n",
    "    .map(lambda file_path: parse(file_path)[0], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to set `output_sequence_length=512` in the `TextVectorization` preprocessing layer since many reviews are shorter than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       233.776720\n",
       "std        173.715418\n",
       "min         10.000000\n",
       "50%        174.000000\n",
       "90%        458.000000\n",
       "max       2470.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for text_batch in train_text:\n",
    "    for text in text_batch:\n",
    "        num_words = len(str(text.numpy()).split())\n",
    "        lengths.append(num_words)\n",
    "\n",
    "pd.Series(lengths).describe(percentiles=[0.5, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining a good vocabulary size (only the most frequent `max_tokens` will be kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121894"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\"\n",
    ")\n",
    "text_vec_layer.adapt(train_text)\n",
    "\n",
    "vocab_size = len(text_vec_layer.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for text_batch in train_text:\n",
    "    for text in text_batch:\n",
    "        standardized_text = re.sub(r\"[^\\w\\s]\", \"\", text.numpy().decode(\"utf-8\").lower())\n",
    "        tokens = str(standardized_text).split()\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "token_counts_df = pd.DataFrame(\n",
    "    list(token_counts.items()), columns=[\"token\", \"count\"]\n",
    ").sort_values(by=\"count\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least half of the tokens only appear once in the training text and 90% of the tokens appear $\\leq$ 23 times, which makes training an embedding for them difficult, therefore it makes sense to clip the vocab to 10,000 of the most frequent tokens out of the ~120,000 total token size.\n",
    "\n",
    "i.e. setting `max_tokens=10_000` in my `TextVectorization` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    121045.000000\n",
       "mean         48.083919\n",
       "std        1539.305968\n",
       "min           1.000000\n",
       "50%           1.000000\n",
       "90%          23.000000\n",
       "max      334706.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts_df[\"count\"].describe(percentiles=[0.5, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    output_sequence_length=512,\n",
    "    max_tokens=10_000,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    ")\n",
    "\n",
    "\n",
    "text_vec_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train, test, & validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_dataset(train=False):\n",
    "    file_paths = [\n",
    "        str(data_dir / f\"{'train' if train else 'test'}/{class_}/*\")\n",
    "        for class_ in [\"pos\", \"neg\"]\n",
    "    ]\n",
    "    return (\n",
    "        tf.data.Dataset.list_files(file_paths, shuffle=True)\n",
    "        .map(parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(\n",
    "            lambda text, label: (text_vec_layer(text), label),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        .cache()\n",
    "        .shuffle(25_000)\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = get_movie_dataset(train=True).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = get_movie_dataset()\n",
    "valid_dataset = test_dataset.take(15_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.skip(15_000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 512), dtype=int64, numpy=\n",
       " array([[  11,    7,   29, ...,    0,    0,    0],\n",
       "        [  10,   26,  434, ...,    0,    0,    0],\n",
       "        [   1,    1, 5322, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  10,   90,  118, ...,    0,    0,    0],\n",
       "        [  33, 6021,    2, ...,    0,    0,    0],\n",
       "        [3885, 2327,    7, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1], dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a model with an `Embedding` layer & then makes predictions based on the mean embedding of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/homl3/lib/python3.10/site-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'lambda' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │     \u001b[38;5;34m5,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m50,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m101\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,090,601</span> (19.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,090,601\u001b[0m (19.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,090,601</span> (19.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,090,601\u001b[0m (19.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def make_text_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(512,)),\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim=text_vec_layer.vocabulary_size(),\n",
    "                output_dim=500,\n",
    "                mask_zero=True,\n",
    "            ),\n",
    "            tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(5):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                100, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_text_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/homl3/lib/python3.10/site-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'lambda_1' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached end learning rate, stopping training\n",
      "Learning rate of minimum loss: 0.0017218656365110064\n"
     ]
    }
   ],
   "source": [
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, starting_lr, end_lr, n_iter):\n",
    "        self.factor = (end_lr / starting_lr) ** (1 / n_iter)\n",
    "        self.starting_lr = starting_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model.optimizer.learning_rate = self.starting_lr\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        lr = self.model.optimizer.learning_rate.numpy() * self.factor\n",
    "        if lr > self.end_lr:\n",
    "            print(\n",
    "                \"\\nReached end learning rate, stopping training\",\n",
    "                f\"Learning rate of minimum loss: {self.learning_rates[np.argmin(self.losses)]}\",\n",
    "                sep=\"\\n\",\n",
    "            )\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        self.model.optimizer.learning_rate = lr\n",
    "        self.learning_rates.append(lr)\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "\n",
    "\n",
    "model = make_text_model()\n",
    "exponential__learning_rate_cb = ExponentialLearningRate(1e-4, 1e-1, 2000)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[exponential__learning_rate_cb],\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(\n",
    "    exponential__learning_rate_cb.learning_rates,\n",
    "    exponential__learning_rate_cb.losses,\n",
    ")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Learning Rate\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_ylim(0.29, 0.69)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/homl3/lib/python3.10/site-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'lambda_2' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "2025-02-24 20:30:03.822286: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2025-02-24 20:30:03.822513: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n",
      "2025-02-24 20:30:03.823470: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m654/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.5585 - loss: 0.6699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 20:30:27.210413: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2025-02-24 20:30:27.210428: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m704/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.5642 - loss: 0.6659"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 20:30:28.960030: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:68] Profiler session collecting data.\n",
      "2025-02-24 20:30:28.978554: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n",
      "2025-02-24 20:30:28.980446: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:147] Collecting XSpace to repository: models/TensorBoard/13/run_2025_02_24_20_30_03/train/plugins/profile/2025_02_24_20_30_28/Edwards-MacBook-Air.local.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5729 - loss: 0.6593 - val_accuracy: 0.8449 - val_loss: 0.4255\n",
      "Epoch 2/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.8298 - loss: 0.3866 - val_accuracy: 0.8604 - val_loss: 0.3302\n",
      "Epoch 3/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.8729 - loss: 0.3002 - val_accuracy: 0.7944 - val_loss: 0.4143\n",
      "Epoch 4/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.8955 - loss: 0.2534 - val_accuracy: 0.7881 - val_loss: 0.4558\n",
      "Epoch 5/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9092 - loss: 0.2290 - val_accuracy: 0.7928 - val_loss: 0.4588\n",
      "Epoch 6/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9208 - loss: 0.2056 - val_accuracy: 0.7813 - val_loss: 0.5398\n",
      "Epoch 7/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9285 - loss: 0.1847 - val_accuracy: 0.8824 - val_loss: 0.3206\n",
      "Epoch 8/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9350 - loss: 0.1699 - val_accuracy: 0.8859 - val_loss: 0.3216\n",
      "Epoch 9/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9333 - loss: 0.1643 - val_accuracy: 0.8739 - val_loss: 0.3384\n",
      "Epoch 10/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9451 - loss: 0.1491 - val_accuracy: 0.7462 - val_loss: 0.8346\n",
      "Epoch 11/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9458 - loss: 0.1430 - val_accuracy: 0.8796 - val_loss: 0.3628\n",
      "Epoch 12/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9486 - loss: 0.1348 - val_accuracy: 0.7866 - val_loss: 0.6098\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    }
   ],
   "source": [
    "model = make_text_model()\n",
    "model.optimizer.learning_rate = 1.5e-3 / 2\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            f\"models/TensorBoard/13/{strftime('run_%Y_%m_%d_%H_%M_%S')}\",\n",
    "            profile_batch=\"650,700\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m train_set\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]\n",
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
