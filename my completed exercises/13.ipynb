{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'runtime_version' from 'google.protobuf' (/opt/miniconda3/lib/python3.12/site-packages/google/protobuf/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/__init__.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     45\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m typing \u001b[38;5;28;01mas\u001b[39;00m npt\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m message\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m full_type_pb2\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/tensorflow/core/framework/attr_value_pb2.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m runtime_version \u001b[38;5;28;01mas\u001b[39;00m _runtime_version\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbol_database \u001b[38;5;28;01mas\u001b[39;00m _symbol_database\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'runtime_version' from 'google.protobuf' (/opt/miniconda3/lib/python3.12/site-packages/google/protobuf/__init__.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The `tf.data` API is useful for reading in data gradually from your hard drive and preprocessing data, it revolves around `tf.data.Dataset` which represents a sequence of data items.\n",
    "\n",
    "2. Splitting the data across multiple files can help with shuffling the data, `tf.data.Dataset.list_files()` automatically shuffles the file paths and the `.interleave()` method with argument `cycle_length > 1` will read from multiple files simultaneously. It can also improve performance, setting `num_parallel_calls=tf.data.AUTOTUNE` TensorFlow will choose the right number of threads dynamically based on the available CPU.\n",
    "\n",
    "3. During training if your GPU utilization is very low, the input pipeline might be the bottleneck. You should call `.prefetch()` so that the dataset has a set number of batches ready to go in memory, or if the whole dataset can fit in memory, call `.cache()` after loading and preprocessing the data, but before shuffling, repeating, batching, and prefetching. Other approaches:\n",
    "    * read and preprocess the data with multiple threads in parallel,\n",
    "    * make sure your preprocessing code is optimized,\n",
    "    * save the dataset into multiple TFRecord files,\n",
    "    * if necessary perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training (TF Transform can help with this), &\n",
    "    * if necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough.\n",
    "\n",
    "4. You can save any binary data to a TFRecord file, however in practice *protobufs* are used.\n",
    "\n",
    "5. The `Example` protobuf format has the advantage that TensorFlow provides some operations to parse it (the `tf.io.parse`*`example()` functions) without you having to define your own format. It is sufficiently flexible to represent instances in most datasets.\n",
    "\n",
    "6. Compressing TFRecord files is useful if they need to be loaded via a network connection, e.g. from AWS S3. However, you shouldn't do this if you don't need to as decompressing the files could slow down training.\n",
    "\n",
    "7. Data preprocessing can take place in three ways:\n",
    "    1. when writing the data files\n",
    "        * **pros:** this will speed up training, the training data may also take up less space e.g. you apply dimensionality reduction\n",
    "        * **cons:** however you must make sure you apply the same preprocessing steps in production, its also not easy to try out different preprocessing steps, also not good for data augmentation\n",
    "    2. within the `tf.data` pipeline\n",
    "        * **pros:** its much easier to experiment with preprocessing steps & data augmentation, multithreading and prefetching can make it very efficient, you can use preprocessing layers in your `tf.data` pipeline and then reuse these layers when deploying your model to production\n",
    "        * **cons:** it will still slow down training, each instance will be preprocessed once per epoch (unless you can use `.cache`), must remember to apply the same preprocessing steps in production\n",
    "    3. preprocessing layers of your model\n",
    "        * **pros:** this is good for inference, as your model will be able to handle raw data, you will not run the risk of mismatch between your training preprocessing & inference preprocessing\n",
    "        * **cons:** it will slow down training, with each instance being processed multiple times\n",
    "\n",
    "8. Categorical features can be encoded using integers if there is a natural ordering or one-hot encoding. For text, where each token is a category, you have far too many categories to use one-hot so embeddings make much more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fashion MNIST data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.a. Writing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "class_labels = (\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    ")\n",
    "\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(\n",
    "    buffer_size=60_000\n",
    ")\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "valid_size = 2048\n",
    "\n",
    "valid_set = train_set.take(valid_size)\n",
    "train_set = train_set.skip(valid_size)\n",
    "\n",
    "\n",
    "def serialize(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    protobuf_example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                \"image\": tf.train.Feature(\n",
    "                    bytes_list=tf.train.BytesList(value=[image_data.numpy()])\n",
    "                ),\n",
    "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return protobuf_example.SerializeToString()\n",
    "\n",
    "\n",
    "data_dir = Path(\"datasets/13/fashion_mnist\")\n",
    "num_shards = 5\n",
    "\n",
    "datasets = {\n",
    "    \"train_set\": train_set,\n",
    "    \"valid_set\": valid_set,\n",
    "    \"test_set\": test_set,\n",
    "}\n",
    "\n",
    "file_paths = dict()\n",
    "for dataset_name, dataset in datasets.items():\n",
    "\n",
    "    dataset_dir = data_dir / dataset_name\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_paths[dataset_name] = [\n",
    "        str(dataset_dir / f\"{dataset_name}-{i}-of-{num_shards}.tfrecord\")\n",
    "        for i in range(1, num_shards + 1)\n",
    "    ]\n",
    "\n",
    "    if not any(dataset_dir.iterdir()):\n",
    "        writers = [\n",
    "            tf.io.TFRecordWriter(file_path) for file_path in file_paths[dataset_name]\n",
    "        ]\n",
    "\n",
    "        for i, (image, label) in dataset.enumerate():\n",
    "            writers[i % num_shards].write(serialize(image, label))\n",
    "\n",
    "        for writer in writers:\n",
    "            writer.close()\n",
    "    else:\n",
    "        print(f\"Skipping {dataset_name} as directory is not empty.\")\n",
    "\n",
    "del (\n",
    "    X_test,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    test_set,\n",
    "    train_set,\n",
    "    valid_set,\n",
    "    data_dir,\n",
    "    dataset,\n",
    "    dataset_dir,\n",
    "    dataset_name,\n",
    "    datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized_example):\n",
    "\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    protobuf_example = tf.io.parse_single_example(\n",
    "        serialized_example, feature_description\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.io.parse_tensor(protobuf_example[\"image\"], out_type=tf.uint8),\n",
    "        tf.cast(protobuf_example[\"label\"], tf.uint8),\n",
    "    )\n",
    "\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "\n",
    "def preprocess(X, y):\n",
    "    return tf.reshape(norm_layer(X), (28, 28, 1)), y\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name, shuffle=False, sample=None):\n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(file_paths[dataset_name], num_parallel_reads=num_shards)\n",
    "        .map(parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(60_000 - valid_size)\n",
    "\n",
    "    if sample:\n",
    "        dataset = dataset.take(sample)\n",
    "\n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_dataset = get_dataset(\"train_set\", shuffle=True, sample=4096)\n",
    "valid_dataset = get_dataset(\"valid_set\")\n",
    "test_dataset = get_dataset(\"test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(\n",
    "    tf.keras.layers.Conv2D,\n",
    "    kernel_size=3,\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "        DefaultConv2D(filters=64, kernel_size=7),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        DefaultConv2D(filters=128),\n",
    "        DefaultConv2D(filters=128),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        DefaultConv2D(filters=256),\n",
    "        DefaultConv2D(filters=256),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=128, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=64, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            f\"models/TensorBoard/13/{strftime('run_%Y_%m_%d_%H_%M_%S')}\",\n",
    "            profile_batch=\"20,120\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Binary classification of the [Large Movie Review Dataset](https://homl.info/imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that parses a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"datasets/13/aclImdb\")\n",
    "\n",
    "\n",
    "def parse(file_path):\n",
    "    content = tf.io.read_file(file_path)\n",
    "\n",
    "    label = tf.strings.regex_replace(file_path, \".*/(pos|neg)/.*\", \"\\\\1\")\n",
    "    label = tf.cast(tf.equal(label, \"pos\"), tf.int32)\n",
    "\n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some basic analysis of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (\n",
    "    tf.data.Dataset.list_files(\n",
    "        [str(data_dir / f\"train/{class_}/*\") for class_ in [\"pos\", \"neg\"]]\n",
    "    )\n",
    "    .map(lambda file_path: parse(file_path)[0], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to set `output_sequence_length=512` in the `TextVectorization` preprocessing layer since many reviews are shorter than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "for text_batch in train_text:\n",
    "    for text in text_batch:\n",
    "        num_words = len(str(text.numpy()).split())\n",
    "        lengths.append(num_words)\n",
    "\n",
    "pd.Series(lengths).describe(percentiles=[0.5, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining a good vocabulary size (only the most frequent `max_tokens` will be kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\"\n",
    ")\n",
    "text_vec_layer.adapt(train_text)\n",
    "\n",
    "vocab_size = len(text_vec_layer.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for text_batch in train_text:\n",
    "    for text in text_batch:\n",
    "        standardized_text = re.sub(r\"[^\\w\\s]\", \"\", text.numpy().decode(\"utf-8\").lower())\n",
    "        tokens = str(standardized_text).split()\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "token_counts_df = pd.DataFrame(\n",
    "    list(token_counts.items()), columns=[\"token\", \"count\"]\n",
    ").sort_values(by=\"count\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least half of the tokens only appear once in the training text and 90% of the tokens appear $\\leq$ 23 times, which makes training an embedding for them difficult, therefore it makes sense to clip the vocab to 10,000 of the most frequent tokens out of the ~120,000 total token size.\n",
    "\n",
    "i.e. setting `max_tokens=10_000` in my `TextVectorization` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts_df[\"count\"].describe(percentiles=[0.5, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    output_sequence_length=512,\n",
    "    max_tokens=10_000,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    ")\n",
    "\n",
    "\n",
    "text_vec_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train, test, & validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_dataset(train=False):\n",
    "    file_paths = [\n",
    "        str(data_dir / f\"{'train' if train else 'test'}/{class_}/*\")\n",
    "        for class_ in [\"pos\", \"neg\"]\n",
    "    ]\n",
    "    return (\n",
    "        tf.data.Dataset.list_files(file_paths, shuffle=True)\n",
    "        .map(parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(\n",
    "            lambda text, label: (text_vec_layer(text), label),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        .cache()\n",
    "        .shuffle(25_000)\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = get_movie_dataset(train=True).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = get_movie_dataset()\n",
    "valid_dataset = test_dataset.take(15_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.skip(15_000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a model with an `Embedding` layer & then makes predictions based on the mean embedding of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def make_text_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(512)),\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim=text_vec_layer.vocabulary_size(),\n",
    "                output_dim=500,\n",
    "                mask_zero=True,\n",
    "            ),\n",
    "            tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(5):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                100, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_text_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, starting_lr, end_lr, n_iter):\n",
    "        self.factor = (end_lr / starting_lr) ** (1 / n_iter)\n",
    "        self.starting_lr = starting_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model.optimizer.learning_rate = self.starting_lr\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        lr = self.model.optimizer.learning_rate.numpy() * self.factor\n",
    "        if lr > self.end_lr:\n",
    "            print(\n",
    "                \"\\nReached end learning rate, stopping training\",\n",
    "                f\"Learning rate of minimum loss: {self.learning_rates[np.argmin(self.losses)]}\",\n",
    "                sep=\"\\n\",\n",
    "            )\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        self.model.optimizer.learning_rate = lr\n",
    "        self.learning_rates.append(lr)\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "\n",
    "\n",
    "model = make_text_model()\n",
    "exponential__learning_rate_cb = ExponentialLearningRate(1e-4, 1e-1, 2000)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[exponential__learning_rate_cb],\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(\n",
    "    exponential__learning_rate_cb.learning_rates,\n",
    "    exponential__learning_rate_cb.losses,\n",
    ")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Learning Rate\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_ylim(0.29, 0.69)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_text_model()\n",
    "model.optimizer.learning_rate = 1.5e-3 / 2\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            f\"models/TensorBoard/13/{strftime('run_%Y_%m_%d_%H_%M_%S')}\",\n",
    "            profile_batch=\"650,700\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]\n",
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
