{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [TensorFlow playground](https://playground.tensorflow.org/)\n",
    "\n",
    "2. Drawing of original ANN (as in figure 10-3) that computes, XOR: $A\\oplus B$. Assuming that a neuron is activated when $\\geq 2$ of its input connections are active.\n",
    "\n",
    "<img width=\"20%\" src=\"datasets/IMG_B049F35CA3D0-1.jpeg\"/><img width=\"20%\" src=\"datasets/IMG_BBB9A8B61AFF-1.jpeg\"/>\n",
    "\n",
    "3. Logistic regression & a classic perceptron, made up of a single layer of threshold logic units (LTUs), function very similarly as classifiers. They differ in two regards:\n",
    "    * Logistic regression uses the sigmoid activation function (or softmax), whereas the perceptron uses a step function. The sigmoid has the benefit of outputting a probability rather than a 1 or 0. It also has a non-zero gradient, which means it can be trained via gradient descent.\n",
    "    * They also differ by their training algorithms. The perceptron training algorithm (a.k.a. Hebian learning) will only converge if the training instances are linearly separable, whereas gradient descent (or SGD) will.\n",
    "\n",
    "4. The sigmoid (or any non-linear) activation function is a key ingredient in training MLPs because it has a non-zero gradient. Meaning MLPs can be trained via gradient descent.\n",
    "\n",
    "5. Sigmoid, Hyperbolic tan, & Rectified Linear Unit are all popular activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    tf.keras.activations.sigmoid,\n",
    "    tf.keras.activations.tanh,\n",
    "    tf.keras.activations.relu,\n",
    "    tf.keras.activations.elu,\n",
    "    tf.keras.activations.hard_sigmoid,\n",
    "    tf.keras.activations.leaky_relu,\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    np.ceil(len(activations) / 3).astype(int),\n",
    "    3,\n",
    "    figsize=(14, 4 * np.ceil(len(activations) / 3).astype(int)),\n",
    ")\n",
    "\n",
    "x = np.linspace(-10, 10, 400)\n",
    "for i, func in enumerate(activations):\n",
    "    axs[i // 3, i % 3].plot(x, func(x))\n",
    "    axs[i // 3, i % 3].set_title(func.__name__)\n",
    "    axs[i // 3, i % 3].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "    * a) X.shape = (m, 10) where m is the number of instances\n",
    "    * b) W_h.shape = (10, 50) & b_h = (50,)\n",
    "    * c) W_o.shape = (50, 3) & b_o = (3,)\n",
    "    * d) Y.shape = (m, 3)\n",
    "    * e) $Y = ReLU(ReLU(XW_h + b_h)W_o + b_o)$\n",
    "7. Classifying emails into ham and spam is binary classification since the classes are mutually exclusive. \n",
    "    * Therefore only **one output neuron** is required, \n",
    "    * **sigmoid** would be a good choice for output layer activation since it outputs values between 0 & 1. \n",
    "\n",
    "    MNIST is a multiclass classification problem, therefore:\n",
    "    * you need one output neuron per class, i.e. **10**,\n",
    "    * **softmax** is an ideal activation function for the output as this returns a probability distribution over the 10 values.\n",
    "\n",
    "    For predicting housing prices, this is a regression problem,\n",
    "    * you only need **one output neuron** since you are only predicting one value, and\n",
    "    * **ReLU** would make sense as it only outputs positive values (passthrough would also work).\n",
    "\n",
    "8. Backpropagation is how neural networks calculate the gradient of the cost function with respect to the model parameters via the chain rule. Reverse-mode autodiff is the highly computationally efficient method used to calculate the partial derivatives (not just in ANNs).\n",
    "\n",
    "9. Hyperparameters of a ANN include: **number of hidden layers**, **size of each of the hidden layers**, the **activation function** of each layer, the learning rate, and batch size. If the model is overfitting you should decrease the number of hidden layers & their size, another effective approach is to use early stopping (a.k.a. shrinking the oversized trousers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training an MLP for the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=10_000, stratify=y_train\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(5, 20, figsize=(15, 3))\n",
    "for ax, digit in zip(axs.flatten(), X_train[:100]):\n",
    "    ax.imshow(digit, cmap=\"binary\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"X_train shape: {X_train.shape}\",\n",
    "    f\"y_train shape: {y_train.shape}\\n\",\n",
    "    f\"X_val shape: {X_val.shape}\",\n",
    "    f\"y_val shape: {y_val.shape}\\n\",\n",
    "    f\"X_test shape: {X_test.shape}\",\n",
    "    f\"y_test shape: {y_test.shape}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    tf.keras.backend.clear_session()  # resets the layer naming\n",
    "\n",
    "    norm_layer = tf.keras.layers.Normalization()\n",
    "    norm_layer.adapt(X_train)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input((28, 28)),\n",
    "            norm_layer,\n",
    "            tf.keras.layers.Flatten(name=\"flatten\"),\n",
    "            tf.keras.layers.Dense(125, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(125, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(125, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(125, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_model()\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"models/10_MNIST_MLP.png\",\n",
    "    show_layer_names=True,\n",
    "    show_shapes=True,\n",
    "    show_layer_activations=True,\n",
    "    dpi=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom callback to find the optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, starting_lr, end_lr, n_iter):\n",
    "        self.factor = (end_lr / starting_lr) ** (1 / n_iter)\n",
    "        self.starting_lr = starting_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model.optimizer.learning_rate = self.starting_lr\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        lr = self.model.optimizer.learning_rate.numpy() * self.factor\n",
    "        if lr > self.end_lr:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        self.model.optimizer.learning_rate = lr\n",
    "        self.learning_rates.append(lr)\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "\n",
    "\n",
    "exponential_learning_rate = ExponentialLearningRate(1e-5, 1e-1, 5_000)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[exponential_learning_rate],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(exponential_learning_rate.learning_rates, exponential_learning_rate.losses)\n",
    "ax.axvline(x=0.003, color=\"r\", linestyle=\"--\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Learning Rate\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the left hand plot we see that the loss starts to rise again at a learning rate of **0.003**, therefore I set the learning rate to 10 times lower that, before retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "\n",
    "model.optimizer.learning_rate = 3e-4\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    verbose=1,\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    grid=True,\n",
    "    xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras Tuner & TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # resets the layer naming\n",
    "\n",
    "\n",
    "def build_model(hyper_params):\n",
    "    num_hidden_layers = hyper_params.Int(\n",
    "        \"num_hidden_layers\", min_value=2, max_value=10, default=4\n",
    "    )\n",
    "    num_neurons = hyper_params.Int(\n",
    "        \"num_neurons\", min_value=16, max_value=512, default=125\n",
    "    )\n",
    "    learning_rate = hyper_params.Float(\n",
    "        \"learning_rate\", min_value=1e-5, max_value=1e-2, default=3e-4, sampling=\"log\"\n",
    "    )\n",
    "    optimizer = hyper_params.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    activation = hyper_params.Choice(\"activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input((28, 28)))\n",
    "    norm_layer = tf.keras.layers.Normalization()\n",
    "    norm_layer.adapt(X_train)\n",
    "    model.add(norm_layer)\n",
    "    model.add(tf.keras.layers.Flatten(name=\"flatten\"))\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(num_neurons, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        print(\"ERROR: no optimizer set\")\n",
    "        return\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "hyperband_tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=10,\n",
    "    factor=5,\n",
    "    hyperband_iterations=4,\n",
    "    overwrite=True,\n",
    "    directory=\"models/10/hyperband_tuning\",\n",
    "    project_name=\"hyperband_tuning\",\n",
    ")\n",
    "\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    \"models/10/hyperband_tuning/tensorboard\"\n",
    ")\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "hyperband_tuner.search(\n",
    "    X_train[:10_000],\n",
    "    y_train[:10_000],\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[tensorboard_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperband_tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining a model with the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(hyperband_tuner.get_best_hyperparameters()[0])\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    \"models/10/tuned_hyper_params_refit/tensorboard\"\n",
    ")\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"models/10/tuned_hyper_params_refit/model.weights.h5\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping_cb, checkpoint_cb, tensorboard_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).argmax(axis=-1)\n",
    "misclassified_mask = y_pred != y_test\n",
    "misclassified_digits = X_test[misclassified_mask]\n",
    "\n",
    "print(\n",
    "    f\"Misclassified {sum(misclassified_mask):,}/{X_test.shape[0]:,} digits in the test set.\"\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(sum(misclassified_mask) // 10, 10, figsize=(15, 70))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(misclassified_digits[i], cmap=\"binary\")\n",
    "    ax.set_title(\n",
    "        f\"predicted: {y_pred[misclassified_mask][i]}\\nactual: {y_test[misclassified_mask][i]}\"\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
