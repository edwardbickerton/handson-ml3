{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import loguniform, mode\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Suppose you've trained 5 different models on the same training data and each achieves a 95% precision. You could combine them using hard voting, wherein the predicted class is given by the majority of the classifiers (or soft voting if all the classifiers have a `predict_proba` method). However, this won't necessarily yield a better result if the models are very similar (i.e. all decision trees with the same hyperparameters).\n",
    "2. Hard voting gives returns the majority vote of the ensemble, whereas soft voting returns the class with the highest average probability by taking an average over each of the estimators predicted class probabilities.\n",
    "3. For bagging & pasting, predictors can be trained in parallel on separate servers to speed up training. A random forest is just an ensemble of decision trees trained via bagging or pasting, therefore the training of random forests can be sped up by distributing it across multiple servers. Boosting involves training weak predictors sequentially so does not benefit from parallelization. For a stacking ensemble, the predictors in each layer are independent of one another so can benefit from parallelization however each layer must be trained sequentially.\n",
    "4. Out-of-bag evaluation: each predictor in a bagging ensemble is evaluated on instances it was not trained on. This makes it possible to have a fairly unbiased evaluation of the ensemble without the need for an additional validation set.\n",
    "5. Extra-trees ensembles don't find the optimal splits, instead splits are made that meet a random threshold. This makes them much faster to train and the randomness introduces an element of regularization.\n",
    "6. If an AdaBoost ensemble underfits the training data, increase the number of estimators or reduce the regularization hyperparameters of the base estimator. You can also try slightly increasing the learning rate.\n",
    "7. If a gradient boosting ensemble overfits the training data you should decrease the learning rate. You could also try early stopping (i.e. using fewer predictors).\n",
    "\n",
    "*bagging vs. pasting* - both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Voting Classification for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", as_frame=False, parser=\"auto\")\n",
    "X, y = mnist.data, mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10_000)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10_000)\n",
    "\n",
    "\n",
    "def plot_digit(image_data):\n",
    "    plt.imshow(image_data.reshape(28, 28), cmap=\"binary_r\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for idx, image_data in enumerate(X_train[:100]):\n",
    "    plt.subplot(10, 10, idx + 1)\n",
    "    plot_digit(image_data)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"X_train.shape: {X_train.shape}\",\n",
    "    f\"X_val.shape: {X_val.shape}\",\n",
    "    f\"X_test.shape: {X_test.shape}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.a. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = GridSearchCV(\n",
    "    KNeighborsClassifier(\n",
    "        p=1,\n",
    "        weights=\"distance\",\n",
    "        algorithm=\"ball_tree\",\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    {\"n_neighbors\": [3, 4, 5, 6]},\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "knn_classifier.fit(X_train[:3_000], y_train[:3_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {\"model\": [], \"val_accuracy\": [], \"y_val_pred\": []}\n",
    "\n",
    "\n",
    "def eval_grid_search(grid_search_model, refit=False):\n",
    "    print(\n",
    "        f\"Best params: {grid_search_model.best_params_}\",\n",
    "        f\"Best score: {grid_search_model.best_score_}\",\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "    y_val_pred = grid_search_model.predict(X_val)\n",
    "    score = accuracy_score(y_val, y_val_pred)\n",
    "    print(\n",
    "        f\"Score on validation set: {score}\",\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "    if refit:\n",
    "        print(\"Refitting on whole training set\")\n",
    "        grid_search_model.best_estimator_.fit(X_train, y_train)\n",
    "        y_val_pred = grid_search_model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"Score on validation set: {score}\")\n",
    "\n",
    "    model_scores[\"model\"].append(grid_search_model.best_estimator_.__class__.__name__)\n",
    "    model_scores[\"val_accuracy\"].append(score)\n",
    "    model_scores[\"y_val_pred\"].append(y_val_pred)\n",
    "\n",
    "\n",
    "eval_grid_search(knn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.b. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = RandomizedSearchCV(\n",
    "    make_pipeline(MinMaxScaler(), SVC()),\n",
    "    {\"svc__C\": loguniform(0.01, 100), \"svc__gamma\": loguniform(0.0001, 1)},\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "svm_classifier.fit(X_train[:3_000], y_train[:3_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_grid_search(svm_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.c. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomizedSearchCV(\n",
    "    RandomForestClassifier(n_jobs=-1),\n",
    "    {\n",
    "        \"n_estimators\": [100 * _ for _ in range(1, 21)],\n",
    "        \"max_features\": loguniform(0.01, 0.05),\n",
    "        \"max_depth\": range(5, 30),\n",
    "    },\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "random_forest_classifier.fit(X_train[:3_000], y_train[:3_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_grid_search(random_forest_classifier, refit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.d. ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_classifier = RandomizedSearchCV(\n",
    "    ExtraTreesClassifier(n_jobs=-1),\n",
    "    {\n",
    "        \"n_estimators\": [100 * _ for _ in range(1, 21)],\n",
    "        \"max_features\": loguniform(0.01, 0.05),\n",
    "        \"max_depth\": range(5, 30),\n",
    "    },\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "extra_tree_classifier.fit(X_train[:3_000], y_train[:3_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_grid_search(extra_tree_classifier, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = np.vstack(model_scores[\"y_val_pred\"]).T\n",
    "y_val_pred = mode(votes.astype(\"int\"), axis=1).mode.astype(\"str\")\n",
    "accuracy_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stacking Classification for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender = RandomizedSearchCV(\n",
    "    make_pipeline(MinMaxScaler(), SVC()),\n",
    "    {\"svc__C\": loguniform(0.01, 100), \"svc__gamma\": loguniform(0.0001, 1)},\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "blender.fit(votes.astype(\"int\"), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
